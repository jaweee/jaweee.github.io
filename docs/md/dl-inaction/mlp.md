### 感知机

1. 收敛定理
2. XOR 错误

### 多层感知机

1. 一层不行多加一层
2. 隐藏的层的大小是一个超参数
3. 激活函数，必须要是非线性的
4. 常用的激活函数：sigmoid、tanh、ReLU （最常用的，简单快速）
5. 多层隐藏：压缩最好是慢慢地递减，也可以 expand 一点再递减

### 模型选择

1. 重点关注泛化误差，也就是在未知情况下的差距，而不是已知训练下的差距
2. 数据集分类，训练数据集和验证数据集、测试数据集
   - 数据集的来源要搞清楚
   - 测试数据集只能用一次，验证数据集用来评估，而且参数也大可能是在这个上面进行调整的
3. K-则交叉验证
   - 由于没有很多数据来用
   - 把数据集分成 K 块，其中取一个座位测试数据集，其他的作为训练数据集，取平均验证集的误差

### 过拟合和欠拟合

数据集和模型容量的不匹配导致的，所以有两种非拟合状态

过拟合：低模型容量和简单数据
欠拟合：高模型容量和复杂数据

模型容量：拟合各种函数的能力，高模型容量是记住所有的训练数据，低模型容量是无法正确拟合数据

泛化误差不是随着模型容量的变化而线性变化的，是有一个最优的泛化误差点的。

1. 在足够大的模型容量的情况下，通过手段来进行降低模型容量，让其到达最优点
2. 有时候需要让步，容乃过拟合，以至让训练误差和测试误差的间隔缩小

### 估计模型容量

1. 参数的个数
2. 参数值的选择范围

### vc

1. 线性分类器的 vc 维，值上是感知机层数加一
2. 可以衡量训练误差和测试误差

### 数据复杂度

1. 样本个数
2. 每个样本的元素个数（256 \* 256）
3. 时间、空间个数（视频）
4. 多样性

### 权重衰退

修改损失函数，变为损失函数+权重 2 的 L2 范数
最广泛的正则化的技术
需要减去一个 L2 范数，所以每次训练都在减小权重的大小
sgd 函数的参数 weight_decay

### 丢弃法

通常只会在训练中使用
丢弃法是一个正则项，在层级加入噪音

上面两种方法都是控制模型复杂度的超参数

### 数据稳定性

梯度爆炸， pow(1.5, 100)， ReLU

梯度消失, pow(0.8, 100)， sigmoid

### 如何让训练更加稳定

目标：让梯度值在合理的范围内，

- 将乘法变加法
- 归一化
- 合理的权重初始化和激活函数

在网络的每一层的输出和梯度都是，均值为 0，方差为一个固定数的随机变量

权重：xavier 初始化，用维度来确定，(input_ndim + output_ndim) / 2

激活函数：tanh、relu、4sigmoid-2

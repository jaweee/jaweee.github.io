# 与学习相关的技巧

**【参数的更新】**

- 随机梯度的缺点是因为其每一步不一定走向最小值，是以之字形往下行进，所以其效率是比较低的
- 介绍了四种优化方法：SGD、Momentum、AdaGrad、Adam(融合了 Momentum 和 AdaGrad)
- 如何选择优化方法还要看具体的问题

**【权重的初始值】**

- 权重的初始值不能完全一致，这样会导致梯度消失
- 要使用高斯分布，但是高斯分布在层数过多的情况下也会导致梯度消失，
- 所以初始值需要 Xavier 分布（1/sqrt(n)）和 He 分布(sqrt(2/n))

**【Batch Normalization】**

- 利用 batch normal 对激活数据（经过某一层进行转换的数据）进行处理，通过期望 u、方差 s 和公式 xi - u / (sqrt(s + c))

**【正则化】**

- 防止过拟合，使用权值衰减法和 Dropout，权值衰减法就是给损失函数添加一个 L2 范数，Dropout 方法就是随机删除神经元

**【超参数的设定】**

- 对于超参数的指定，给定超参数的一个 10 的幂次范围，在其中不断随机采样，通过验证数据评估，最后重复前面的步骤，除了上述的实验性操作，也可以考虑贝叶斯最优化
